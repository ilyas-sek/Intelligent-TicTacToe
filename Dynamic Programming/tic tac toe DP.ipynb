{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9cHaxhvuziR",
        "tags": []
      },
      "source": [
        "GOALS:\n",
        "<li>Implementation of dynamic programming algorithms such as policy iteration and value iteration.</li>\n",
        "<li>Applying dynamic programming algorithms to the Tic Tac Toe game to find the optimal strategie. </li>\n",
        "<li>Comparing results. </li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsE34FbquziS"
      },
      "source": [
        "### <font color='red'> 1. Defining the environment,states, actions and rewards </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSLFZII5uziT"
      },
      "source": [
        "In order to implement a Tic Tac Toe game where a player can play against the computer, we will consider the problem as an MDP. We will define its principal components such as the environment, states, actions, and rewards and then apply dynamic programming algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hwJcjc6uziU"
      },
      "source": [
        "##### <font color='green'> 1. States </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8me7Yg3uziV"
      },
      "source": [
        "We define a state in our MDP model as a board configuration of the game. Our goal is to let the agent behave optimally while playing with 'O' or 'X'. We choose to train our agent for 'O', and then we can deal with 'X' case by exchanging roles. <br>\n",
        "We represent a state as a 9-character string (a string with a length of 9) containing 'X','O' and ' ' for empty cases.\n",
        "We put all the states in an array, which contains all possible states that an agent can take an action in and some terminal actions, conditional on the agent playing with 'O'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLKZ7-2SuziV"
      },
      "source": [
        "  First, we define a function that takes a state and decides whether this state is valid or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lfN-35YduziW"
      },
      "outputs": [],
      "source": [
        "def is_valid_board(board):\n",
        "    # Check if the board has exactly 9 characters\n",
        "    if len(board) != 9:\n",
        "        return False\n",
        "\n",
        "    # Count the number of Xs and Os\n",
        "    count_x = board.count('X')\n",
        "    count_o = board.count('O')\n",
        "\n",
        "    # Check if the difference in counts of X and O is at most 1\n",
        "    if abs(count_x - count_o) > 1:\n",
        "        return False\n",
        "\n",
        "    # Check if the number of O is than X\n",
        "    if count_o > count_x :\n",
        "        return False\n",
        "\n",
        "    # Check for invalid characters\n",
        "    valid_characters = set(['X', 'O', ' '])\n",
        "    if any(char not in valid_characters for char in board):\n",
        "        return False\n",
        "\n",
        "    # Check if the game is already won by one of the players\n",
        "    if is_winner(board, 'X') and is_winner(board, 'O'):\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "def is_winner(board, player):\n",
        "    # Check rows, columns, and diagonals for a win\n",
        "    win_conditions = [\n",
        "        [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n",
        "        [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n",
        "        [0, 4, 8], [2, 4, 6]              # Diagonals\n",
        "    ]\n",
        "\n",
        "    for condition in win_conditions:\n",
        "        if all(board[i] == player for i in condition):\n",
        "            return True\n",
        "\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIfR-l5buziX"
      },
      "source": [
        "Next, we create a function that can recursively generate all possible 9-character strings ($3^9$ states) containing 'X', 'O', and ' ', while preserving only valid states (boards) using the \"is_valid_board\" function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "r9-RXfbquziY",
        "outputId": "862a148d-0115-4a59-8d97-ad1143fcc08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of valid states is :  5890\n"
          ]
        }
      ],
      "source": [
        "# Generate all valid board configurations\n",
        "valid_states = set()\n",
        "empty_board = '         '  # 9 empty spaces\n",
        "\n",
        "def generate_states(board, index=0):\n",
        "    if index == 9:\n",
        "        if is_valid_board(board):\n",
        "            valid_states.add(board)\n",
        "        return\n",
        "\n",
        "    # Recursively try all possible moves for X and O\n",
        "    for char in 'XO ':\n",
        "        new_board = board[:index] + char + board[index + 1:]\n",
        "        generate_states(new_board, index + 1)\n",
        "\n",
        "generate_states(empty_board)\n",
        "\n",
        "# Convert the set of valid states into a list\n",
        "valid_states_list = list(valid_states)\n",
        "\n",
        "print(\"Number of valid states is : \",len(valid_states_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA7SXUxQuziZ"
      },
      "source": [
        "These 5890 states involve all possible states that a player (supposed to play with 'O') can encounter in a real game, including some terminal states (end game scenarios) such as a winning case or a draw when the numbers of 'O' and 'X' are equal on the board. These are useful states that will be used later for evaluating the rest of the non-terminal states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vb_bCfa6uzia"
      },
      "source": [
        "#### <font color='green'> 2. Actions </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvYUhl0Kuzib"
      },
      "source": [
        "An action is represented as an integer comprising a value between 0 and 8. This integer is the index of an empty space in the 9-characters string (on the board)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo3tMJUPuzib"
      },
      "source": [
        "#### <font color='green'> 3. Environment </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMcDBSsJuzib"
      },
      "source": [
        "We define our game environment with a class called TicTacToe. This class has two attributes and some useful functions to ensure efficient gameplay while maintaining the rules and logic of the Tic Tac Toe game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CnnuvlFXuzib"
      },
      "outputs": [],
      "source": [
        "class TicTacToe:\n",
        "    def __init__(self, initial_board=None):\n",
        "        if initial_board is None:\n",
        "            self.board = [' ' for _ in range(9)]  # Initialize an empty 3x3 grid\n",
        "        else:\n",
        "            # Verify that the provided initial_board is a valid 9-character string\n",
        "            if len(initial_board) == 9 and all(char in ('X', 'O', ' ') for char in initial_board):\n",
        "                self.board = list(initial_board)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid initial board configuration\")\n",
        "\n",
        "        self.current_player = 'X'  # Start with player 'X'\n",
        "    def define_current_player(self,player) :\n",
        "        self.current_player=player\n",
        "\n",
        "    def get_board_config(self):\n",
        "        return \"\".join(self.board)\n",
        "\n",
        "    def set_board_from_config(self,config):\n",
        "        for i in range(9):\n",
        "            self.board[i]=config[i]\n",
        "        return True\n",
        "\n",
        "    def display_board(self):\n",
        "        for i in range(3):\n",
        "            for j in range(3):\n",
        "                print(self.board[i * 3 + j], end=\"\")\n",
        "\n",
        "                if j < 2:\n",
        "                    print(\" | \", end=\"\")\n",
        "\n",
        "            print()  # Move to the next row\n",
        "            if i < 2:\n",
        "                print(\"---------\")  # Display horizontal dividers between rows\n",
        "\n",
        "    def get_all_possible_next_states(self,player):\n",
        "        empty_cells = [i for i, char in enumerate(self.board) if char == ' ']\n",
        "        next_states = []\n",
        "        for cell in empty_cells:\n",
        "            next_state = self.board.copy()\n",
        "            next_state[cell] = player\n",
        "            next_states.append(\"\".join(next_state))\n",
        "        return next_states\n",
        "\n",
        "    def get_possible_moves(self):\n",
        "        empty_cells=[]\n",
        "        game,winner=self.is_game_over()\n",
        "        if game!=True :\n",
        "            for i in range(9):\n",
        "                if self.board[i]==' ' :\n",
        "                    empty_cells.append(i)\n",
        "            return empty_cells\n",
        "        return None\n",
        "    def make_move(self, position):\n",
        "        if self.board[position] == ' ':\n",
        "            self.board[position] = self.current_player\n",
        "            # Toggle the current player for the next turn\n",
        "            self.current_player = 'X' if self.current_player == 'O' else 'O'\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Invalid move. Try again. \")\n",
        "            return False\n",
        "\n",
        "    def is_game_over(self):\n",
        "        # Check for win conditions\n",
        "        # Check rows, columns, and diagonals for three in a row\n",
        "        win_conditions = [\n",
        "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n",
        "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n",
        "            [0, 4, 8], [2, 4, 6]  # Diagonals\n",
        "        ]\n",
        "\n",
        "        for condition in win_conditions:\n",
        "            a, b, c = condition\n",
        "            if (\n",
        "                self.board[a] == self.board[b] == self.board[c] != ' '\n",
        "            ):\n",
        "                return True, self.board[a]\n",
        "\n",
        "        # Check for a draw (no empty spaces left)\n",
        "        if ' ' not in self.board:\n",
        "            return True, None  # Game is a draw\n",
        "\n",
        "        return False, None  # Game is not over yet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb78LMXRuzic"
      },
      "source": [
        "#### <font color='green'> 4. Rewards </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW1Sd_VMuzid"
      },
      "source": [
        "For the rewards we define a function that takes a state and return its reward as follow :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1pR9bPZHuzid"
      },
      "outputs": [],
      "source": [
        "def Reward(board_config) :\n",
        "    TicTacToeEnv=TicTacToe(board_config)\n",
        "    game,winner=TicTacToeEnv.is_game_over()\n",
        "    if winner=='O' :\n",
        "        return 500\n",
        "    elif winner=='X' :\n",
        "        return -3500\n",
        "    else :\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXWYKVlDuzid"
      },
      "source": [
        "### <font color='red'> 2. Policy iteration implementation  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6cNOiY6uzie"
      },
      "source": [
        "Our goal in the Policy Iteration algorithm is to find an optimal policy that maps each state to the optimal actions using the calculation of the value function. <br><br>\n",
        "Initially, an arbitrary policy is initialized, and the value function of all states is set to 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zlGXm8Aluzie"
      },
      "outputs": [],
      "source": [
        "# Initialize the value function for each state to 0\n",
        "value_function = {state: 0 for state in valid_states_list}\n",
        "\n",
        "policy={} # Initialize an empty dictionary for the policy\n",
        "\n",
        "for state in valid_states_list:\n",
        "    legal_actions = [i for i, char in enumerate(state) if char == ' ']\n",
        "    if is_winner(state,'X') or is_winner(state,'O') :\n",
        "        policy[state]=None  # If the current state is a winning state, set the policy to None (No action to be taken)\n",
        "    else :\n",
        "        legal_actions = [i for i, char in enumerate(state) if char == ' ']\n",
        "        if legal_actions:\n",
        "            policy[state] = legal_actions[0]\n",
        "        else :                      # If there are no legal actions available, set the policy to None (draw state)\n",
        "            policy[state] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgT3eM-1uzie"
      },
      "source": [
        "To implement the policy iteration algorithm, we first need to implement two functions:\n",
        "\n",
        "<li>Policy evaluation: This function evaluates the current policy given as input and returns its value function.</li>\n",
        "<li>Policy improvement: This function takes a policy and its value function as input and returns a better policy than the one given.</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hmwke2ZWuzif"
      },
      "outputs": [],
      "source": [
        "def policy_evaluation(policy, value_function):\n",
        "    tic_tac_toe_env = TicTacToe()\n",
        "    gamma = 0.8\n",
        "    delta = 1e-8  # A small threshold for convergence\n",
        "    num_iteration=0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for board_config in value_function:\n",
        "            old_value = value_function[board_config]\n",
        "            tic_tac_toe_env.set_board_from_config(board_config)\n",
        "\n",
        "            action = policy[board_config]\n",
        "\n",
        "            if action is not None:  # Only update non-terminal states with legal actions\n",
        "                tic_tac_toe_env.define_current_player('O')\n",
        "                tic_tac_toe_env.make_move(action)\n",
        "                board_config_after_move=tic_tac_toe_env.get_board_config()\n",
        "                successor_states = tic_tac_toe_env.get_all_possible_next_states(\"X\")\n",
        "\n",
        "                # Initialize the new value for this state\n",
        "                new_value = 0\n",
        "\n",
        "                # Check if the current state is a terminal state (win or draw)\n",
        "                if is_winner(board_config_after_move, 'X') or is_winner(board_config_after_move, 'O') or ' ' not in board_config_after_move:\n",
        "                    # Set the value to the reward of the terminal state\n",
        "                    new_value = Reward(board_config_after_move)\n",
        "                else:\n",
        "                    # Iterate over all possible successor states\n",
        "                    num_successor_states = len(successor_states)\n",
        "\n",
        "                    #we suppose that the envirement responds uniformly random,\n",
        "                    #so for each next state s' : p(s'|s,a)=1/(Number of possible moves by the opponent.)\n",
        "                    for successor_config in successor_states:\n",
        "                        new_value += (1.0 / num_successor_states) * (Reward(successor_config) + gamma * value_function[successor_config])\n",
        "\n",
        "                value_function[board_config] = new_value\n",
        "            delta = max(delta, abs(old_value - value_function[board_config]))\n",
        "        num_iteration+=1\n",
        "        if delta < 1e-8:\n",
        "            return num_iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qC7GgCthuzif"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(policy, value_function):\n",
        "    tic_tac_toe_env=TicTacToe()\n",
        "    gamma=0.8\n",
        "    policy_stable = True\n",
        "    for board_config in policy:  # Iterate over all possible board configurations\n",
        "        old_action = policy[board_config]\n",
        "        if old_action is not None:  # Skip Terminal state\n",
        "            best_action = None\n",
        "            best_value = -float('inf')\n",
        "            tic_tac_toe_env.set_board_from_config(board_config)\n",
        "            for action in tic_tac_toe_env.get_possible_moves():\n",
        "                # Make a move and calculate the expected value\n",
        "                tic_tac_toe_env.define_current_player('O')\n",
        "                tic_tac_toe_env.make_move(action)\n",
        "                board_config_after_move=tic_tac_toe_env.get_board_config()\n",
        "                successor_states = tic_tac_toe_env.get_all_possible_next_states(\"X\")\n",
        "                 # Initialize the new value for this state\n",
        "                expected_value = 0\n",
        "                # Check if the current state is a terminal state (win or draw)\n",
        "\n",
        "                if is_winner(board_config_after_move, 'X') or is_winner(board_config_after_move, 'O') or ' ' not in board_config_after_move:\n",
        "                    # Set the value to the reward of the terminal state\n",
        "                    expected_value = Reward(board_config_after_move)\n",
        "                else:\n",
        "                    # Iterate over all possible successor states\n",
        "                    num_successor_states = len(successor_states)\n",
        "                    for successor_config in successor_states:\n",
        "                        expected_value += (1.0 / num_successor_states) * (Reward(successor_config) + gamma * value_function[successor_config])\n",
        "\n",
        "\n",
        "                if expected_value > best_value:\n",
        "                    best_action = action\n",
        "                    best_value = expected_value\n",
        "                tic_tac_toe_env=TicTacToe(board_config)\n",
        "\n",
        "            policy[board_config] = best_action\n",
        "\n",
        "        if old_action != policy[board_config]:\n",
        "            policy_stable = False\n",
        "\n",
        "    return policy_stable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_0mY3Duzig"
      },
      "source": [
        "Now we define the policy iteration function that takes an arbitray policy and return the optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FsH8gKm_uzig"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(policy, value_function):\n",
        "    num_iter=0\n",
        "    policy_stable = False\n",
        "    while not policy_stable:\n",
        "\n",
        "        # Policy evaluation\n",
        "        policy_evaluation(policy,value_function)\n",
        "\n",
        "        # Policy improvement\n",
        "        policy_stable = policy_improvement(policy,value_function)\n",
        "        value_function = {state: 0 for state in valid_states_list}\n",
        "        # Update the value function\n",
        "        num_iter+=1\n",
        "    print(\"The number of iterations needed to converge is : \",num_iter)\n",
        "    return policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLkK0ymJuzih"
      },
      "source": [
        "We execute policy_iteration to get the optimal policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ptOB8ZDUuzih",
        "outputId": "e364355b-4166-4dd5-e6c4-26d0cd81042a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of iterations needed to converge is :  4\n"
          ]
        }
      ],
      "source": [
        "num_iter=policy_iteration(policy,value_function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPsetjoIuzii"
      },
      "source": [
        "Results  :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekbohROtuzii"
      },
      "source": [
        "To test the performance of the policy we got, we create a function named game_tester. This function takes a policy and the number of games as arguments, then simulates the desired number of games and returns the win rate, loss rate, and draw rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W8FtybjIuzii",
        "outputId": "2c4c3354-f891-4825-9d0b-6340800ec6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "if the oppenent starts first : \n",
            "Win Rate: 91.62%\n",
            "Loss Rate: 0.00%\n",
            "Draw Rate: 8.38%\n",
            "if the agent starts first : \n",
            "Win Rate: 99.47%\n",
            "Loss Rate: 0.00%\n",
            "Draw Rate: 0.53%\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def game_tester(policy,turn, number_of_games=1000):\n",
        "    win_count = 0\n",
        "    loss_count = 0\n",
        "    draw_count = 0\n",
        "    player_turn=turn\n",
        "    for _ in range(number_of_games):\n",
        "        tic_tac_toe_env = TicTacToe()\n",
        "        game_over = False\n",
        "\n",
        "        while not game_over:\n",
        "            current_player = 'X' if player_turn % 2 == 0 else 'O'\n",
        "\n",
        "            if current_player == 'X':\n",
        "                possible_moves = tic_tac_toe_env.get_possible_moves()\n",
        "                move = random.choice(possible_moves)\n",
        "            else:\n",
        "                move = policy[tic_tac_toe_env.get_board_config()]\n",
        "\n",
        "            tic_tac_toe_env.define_current_player(current_player)\n",
        "            tic_tac_toe_env.make_move(move)\n",
        "            player_turn += 1\n",
        "\n",
        "            game_over, winner = tic_tac_toe_env.is_game_over()\n",
        "        player_turn=turn\n",
        "        if winner == 'X':\n",
        "            loss_count += 1\n",
        "        elif winner == 'O':\n",
        "            win_count += 1\n",
        "        else:\n",
        "            draw_count += 1\n",
        "\n",
        "    rate_win = win_count / number_of_games\n",
        "    rate_loss = loss_count / number_of_games\n",
        "    rate_draw = draw_count / number_of_games\n",
        "\n",
        "    print(\"Win Rate: {:.2%}\".format(rate_win))\n",
        "    print(\"Loss Rate: {:.2%}\".format(rate_loss))\n",
        "    print(\"Draw Rate: {:.2%}\".format(rate_draw))\n",
        "\n",
        "# Call the game_tester function\n",
        "print(\"if the oppenent starts first : \")\n",
        "game_tester(policy,0,number_of_games=100000)\n",
        "print(\"if the agent starts first : \")\n",
        "game_tester(policy,1,number_of_games=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKHAArPRp_oG"
      },
      "source": [
        "To simulate a game between a human and an agent, we create a 'launch_game' function that takes a policy defining how the agent will behave as an argument and launches a game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dbili58Vuzij"
      },
      "outputs": [],
      "source": [
        "def launch_game(policy,turn_count=0):\n",
        "    tic_tac_toe_env = TicTacToe()\n",
        "    game_over = False\n",
        "\n",
        "    while not game_over:\n",
        "        current_player = 'X' if turn_count % 2 == 0 else 'O'\n",
        "        if current_player=='X' :\n",
        "            print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
        "            print(\"Your turn : \\n\")\n",
        "            tic_tac_toe_env.display_board()\n",
        "\n",
        "        if current_player == 'X':\n",
        "            move = int(input(\"Enter a position (between 0 and 8) : \"))\n",
        "        else:\n",
        "            move = policy[tic_tac_toe_env.get_board_config()]\n",
        "\n",
        "        tic_tac_toe_env.define_current_player(current_player)\n",
        "        if tic_tac_toe_env.make_move(move):\n",
        "            turn_count += 1\n",
        "\n",
        "        game_over, winner = tic_tac_toe_env.is_game_over()\n",
        "    if winner == 'X':\n",
        "        print(\"You won!\\n\\n\")\n",
        "    elif winner == 'O':\n",
        "        print(\"You lost!\\n\\n\")\n",
        "    else:\n",
        "        print(\"It's a draw!\\n\\n\")\n",
        "    tic_tac_toe_env.display_board()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kNsNvOXcuzij"
      },
      "outputs": [],
      "source": [
        "#lanch_game(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly16LyW5uzik"
      },
      "source": [
        "### <font color='red'> 3. Value iteration implementation  </font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IpS3ZI6fuzik"
      },
      "outputs": [],
      "source": [
        "def value_iteration(policy,value_function):\n",
        "    gamma = 0.9  # Discount factor\n",
        "    delta_threshold = 1e-8  # A small threshold for convergence\n",
        "    num_iter=0\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for board_config in value_function:\n",
        "            old_value = value_function[board_config]\n",
        "\n",
        "            best_value = -float('inf')\n",
        "            tic_tac_toe_env = TicTacToe()\n",
        "            tic_tac_toe_env.set_board_from_config(board_config)\n",
        "\n",
        "            if policy[board_config]!=None :\n",
        "                for action in tic_tac_toe_env.get_possible_moves():\n",
        "                    # Make a move and calculate the expected value\n",
        "                    tic_tac_toe_env.define_current_player('O')\n",
        "                    tic_tac_toe_env.make_move(action)\n",
        "                    board_config_after_move = tic_tac_toe_env.get_board_config()\n",
        "\n",
        "                    successor_states = tic_tac_toe_env.get_all_possible_next_states(\"X\")\n",
        "\n",
        "                    # Initialize the new value for this state\n",
        "                    expected_value = 0\n",
        "\n",
        "                    # Check if the current state is a terminal state (win or draw)\n",
        "                    if is_winner(board_config_after_move, 'X') or is_winner(board_config_after_move, 'O') or ' ' not in board_config_after_move:\n",
        "                        # Set the value to the reward of the terminal state\n",
        "                        expected_value = Reward(board_config_after_move)\n",
        "                    else:\n",
        "                        # Iterate over all possible successor states\n",
        "                        num_successor_states = len(successor_states)\n",
        "                        for successor_config in successor_states:\n",
        "                            expected_value += (1.0 / num_successor_states) * (\n",
        "                                    Reward(successor_config) + gamma * value_function[successor_config])\n",
        "\n",
        "                    if expected_value > best_value:\n",
        "                        best_value = expected_value\n",
        "                    tic_tac_toe_env=TicTacToe(board_config)\n",
        "                value_function[board_config] = best_value\n",
        "                delta = max(delta, abs(old_value - value_function[board_config]))\n",
        "        num_iter+=1\n",
        "        if delta < delta_threshold:\n",
        "            break\n",
        "\n",
        "    # Policy improvement after value iteration\n",
        "    policy_improvement(policy,value_function) #after calculating vpi* we execute policy improvement for choosing the greedy policy\n",
        "    print(\"The number of iterations needed to converge is : \",num_iter)\n",
        "    return policy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ7xLaZFrm1b"
      },
      "source": [
        "Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Lgqz5In6uzil"
      },
      "outputs": [],
      "source": [
        "# Initialize the value function for each state to 0\n",
        "value_function = {state: 0 for state in valid_states_list}\n",
        "\n",
        "policy_2={} # Initialize an empty dictionary for the\n",
        "\n",
        "for state in valid_states_list:\n",
        "    legal_actions = [i for i, char in enumerate(state) if char == ' ']\n",
        "    if is_winner(state,'X') or is_winner(state,'O') :\n",
        "        policy_2[state]=None  # If the current state is a winning state, set the policy to None (No action to be taken)\n",
        "    else :\n",
        "        legal_actions = [i for i, char in enumerate(state) if char == ' ']\n",
        "        if legal_actions:\n",
        "            policy_2[state] = legal_actions[0]\n",
        "        else :                      # If there are no legal actions available, set the policy to None (draw state no ation to be taken)\n",
        "            policy_2[state] = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8W3V277rrhv"
      },
      "source": [
        "Train policy_2 using the value iteration function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zbgUpV6Auzil",
        "outputId": "6f0e2ef0-3793-4daa-dce0-5bbc45e6c5f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of iterations needed to converge is :  5\n"
          ]
        }
      ],
      "source": [
        "n=value_iteration(policy_2,value_function)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0mvkh_dduzix",
        "outputId": "013b3009-23a3-4257-91e2-880431caf014"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "if the oppenent starts first : \n",
            "Win Rate: 91.65%\n",
            "Loss Rate: 0.00%\n",
            "Draw Rate: 8.35%\n",
            "if the agent starts first : \n",
            "Win Rate: 99.49%\n",
            "Loss Rate: 0.00%\n",
            "Draw Rate: 0.51%\n"
          ]
        }
      ],
      "source": [
        "# Call the game_tester function\n",
        "print(\"if the oppenent starts first : \")\n",
        "game_tester(policy_2,0,number_of_games=100000)\n",
        "print(\"if the agent starts first : \")\n",
        "game_tester(policy_2,1,number_of_games=100000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boWFyPeJ6FmI"
      },
      "source": [
        "In the end, we will save our agent's policy in a JSON file for future use. The agent is currently trained to use 'O' optimally, but we also want to save an agent that can use 'X' optimally. To achieve this, we will exchange roles between 'X' and 'O' in each state while preserving the same actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "27qpA2CJFKGT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def convert_state(state):\n",
        "    s=list(state)\n",
        "    for i in range(len(state)):\n",
        "        if state[i]=='X' :\n",
        "            s[i]='O'\n",
        "        elif state[i]=='O' :\n",
        "            s[i]='X'\n",
        "        else :\n",
        "            s[i]=state[i]\n",
        "    return \"\".join(s)\n",
        "\n",
        "policy_3={convert_state(state) : policy_2[state] for state in policy_2}\n",
        "\n",
        "# Saving the dictionary to a file\n",
        "with open('Optimal_strategie_DP.json', 'w') as f:\n",
        "    json.dump(policy_3, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6p9hnIBsK9A"
      },
      "source": [
        "### <font color='red'> 4. Conclusion:  </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF-nQYLNsWmS"
      },
      "source": [
        "Value iteration and policy iteration are both efficient algorithms that quickly converge, making them valuable for reinforcement learning, including in complex games like Tic-Tac-Toe."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
